# LDD3 Chapter 7 Notes

## Chapter 7: Time, Delays, and Deferred Work

Chapter goal: Understand how timing works in drivers. 

Dealing with time in drivers involves the following:

- Measuring time lapses and comparing times 
- Knowing the current time
- Delaying operation for a specified amount of time
- Scheduling asynchronous functions to happen at a later time

### Measuring Time Lapses

The kernel keeps track of time by using timer interrupts (covered more in chapter 10). Timer interrupts are generated by the system's timing hardware at regular intervals. This is programmed at boot time by the kernel according to the value of `HZ`. This value is architecture dependent and defined in `<linux/param.h>`. Default values range from 50 to 1200 ticks per second on real hardware, and can be as slow as 24 for virtual machines. Most common speeds are at 100 or 1000 interrupts per second. 

A higher `HZ` value will lead to a more fine-grained resolution for asynchronous tasks, but will add extra overhead with needed to deal with extra timer interrupts. Just leave it at the default value and everything should work. If you change `HZ` you need to recompile the kernel. 

Every time a timer interrupt occurs, the value of an internal kernel variable is incremented. This counter is initialized to 0 at boot, so it represents the number of clock ticks since last boot. The variable is called `jiffies_64`. Usually drivers access the `jiffies` variable instead. The `jiffies` variable is of type unsigned long and is the same as `jiffies_64` (or at least its significant bits are). Accessing `jiffies` is faster and accesses to `jiffies_64` are not guaranteed to be atomic on all architectures. Some CPU platforms also have a high-res counter that software can read. 

Side note: Jiffy sounds totally rediculous to me. Here is an article that describes jiffies in more detail:
[Jiffies Article Link](http://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch10lev1sec3.html)

### Using the jiffies Counter

The functions to read the jiffies value are found in `<linux/jiffies.h>` but simply including `<linux/sched.h>` will also pull in the jiffies header file. The jiffies variables should be considered READ-ONLY.

An example of using the jiffies variable:

```c
#include <linux/jiffies.h>
unsigned long j, stamp_1, stamp_half, stamp_n;

j = jiffies; /* read the current value */
stamp_1 = j + HZ; /* 1 second in the future */
stamp_half = j + HZ/2; /* half a second */
stamp_n = j + n * HZ / 1000; /* n milliseconds */
```

This code will have no problem with jiffies wrapping around. There are some functions to help with proper calculation:

```c
#include <linux/jiffies.h>

int time_after(unsigned long a, unsigned long b);
int time_before(unsigned long a, unsigned long b);
int time_after_eq(unsigned long a, unsigned long b);
int time_before_eq(unsigned long a, unsigned long b);
```

These are pretty self-explanatory in the documentation, but there is also an easy way to write it out with proper casting as:

```c
//Calculate different in units of jiffies
diff = (long)t2 - (long)t1;

// Then convert to milliseconds with:
msec = diff * 1000 / HZ;
```

Sometimes you need to exchange time with user space time structs (two popular ones are `timespec` and `timeval`). The kernel has 4 helper functions for this:

```c
#include <linux/time.h>

unsigned long timespec_to_jiffies(struct timespec *value);
void jiffies_to_timespec(unsigned long jiffies, struct timespec *value);
unsigned long timeval_to_jiffies(struct timeval *value);
void jiffies_to_timeval(unsigned long jiffies, struct timeval *value);
```

If you need to access the 64-bit jiffies, the kernel exports a special helper function to properly lock the variable:

```c
#include <linux/jiffies.h> //found in this file

u64 get_jiffies_64(void);
```

Note: Actual clock frequency is hidden from user space. The macro `HZ` usually expands to 100 when users include `param.h`. This applies to clock(3), times(2), and any related function. You can see the number of interrupts with

```bash
cat /proc/interrupts | grep LOC
```

and view your total uptime with:

```bash
cat /proc/uptime 
```

### Processor-Specific Registers

A lot of processors will have a special register that counts individual clock ticks. This can overflow and you should be able to handle these events. One popular counter register is the TSC, timestamp counter, that counts individual clock cycles and can be read from user space as well as kernel space. After including `<asm/msr.h>` you can use one of the following macros for reading clock ticks:

```c
rdtsc(low32,high32);
rdtscl(low32);
rdtscll(var64);
```

The first macro atomically reads the 64 bit value into two 32-bit variables. The second one reads the low half of the register into a 32 bit variable, and the third reads the value into a long long variable. All of the macros store values into their arguments. Sometimes only reading the low half is sufficient - for example a 1 GHz CPU will overflow on this only once every 4.2 seconds. However, many modern CPUs are now hitting the 5 GHz mark, so this may not be a true statement anymore.

One example of only using the lower half of the register is to measure the time of an execution itself with:

```c
unsigned long ini, end;
rdtscl(ini); rdtscl(end);
printk("time lapse: %li\n", end - ini);
```

Kernel headers also offer an architecture-independent method to get the number of clock cycles. The method name is `get_cycles` and is defined in `<asm/timex.h>`. The prototype is:

```c
#include <linux/timex.h>
cycles_t get_cycles(void);
```

- the `cycles_t` type is an appropriate unsigned type to hold the value read

### Knowing the Current Time 

Using `jiffies` is usually sufficient for most time interval measurements like those needed for telling the difference between single and double clicks of a mouse. If you need precise measement for short time lapses, processor-specific registers are needed.

There is a function that returns the wall-clock time into a jiffies value, but this should be avoided. Here is the code that would do it:

```c
#include <linux/time.h>
unsigned long mktime (unsigned int year, unsigned int mon,
                      unsigned int day, unsigned int hour,
                      unsigned int min, unsigned int sec);
```

Sometimes you still need an absolute timestamp in kernel space. In the kernel header `<linux/time.h>` is the do_gettimeofday function. When called, it filles a `struct timeval` pointer which has values for seconds and milliseconds. The prototype for do_gettimeofday is:

```c
#include <linux/time.h>
void do_gettimeofday(struct timeval *tv);
```

This is supposed to have a resolution in the millisecond range because it can ask the timing hardware what fraction of a jiffy has passed. The current time is also available from the xtime variable which is a `struct timespec` value. Direct use of this variable is discouraged because it is hard to atomically access both the fields at once. As a result, the kernel offers the utility function current_kernel_time with prototype:

```c
#include <linux/time.h>
struct timespec current_kernel_time(void);
```

### Delaying Execution

Sometimes we need to delay the execution of a piece of code, usually to allow the hardware to finish something up. We have several ways of doing this in the kernel, but first a few considerations:

- long delays will be multiple jiffies long
- short delays are implemented with software loops
- the in-between area is a little more confusing

#### Long Delays

This is for delays that are longer than one clock tick. One simple way is with busy waiting (think spinlocks). The implementation is to wait a multiple of clock ticks. This method is not reccomended, but is super simple. Here is the code:

```c
while (time_before(jiffies, j1))  //j1 is the value of jiffes at the expiration of the delay
    cpu_relax();
```

- `cpu_relax() invokes an architecture-specific way of saying that you’re not doing much with the processor at the moment.`

Busy waits can be expensive for computers. There is also the issue of the process being interrupted during its delay, scheduling other processes and making the delay a minimum, rather than an absolute value. 

We can also explicitly release the CPU when not using it by yielding the processor. An example is shown below:

```c
while (time_before(jiffies, j1)) {
    schedule( );
}
```

We have now alleviated the busy wait issue of hogging the processor and doing nothing, but we have not yet completely solved the issue of timing. This method is  not great because even when you schedule(), there is no guarantee that your process will start right away. 

The delay loops shown above work by watching the jiffy counter without telling anyone. The actual best way to implement a delay is to have the kernel do it for you (go kernel!). There are two ways to implement jiffy-based timeouts, depending on if your driver is waiting for something else or not. If your driver uses a wait queue to wait for something else, but you want to be sure that it runs in a certain period of time, it can use wait_event_timeout or wait_event_interruptible_timeout with prototypes shown below:

```c
#include <linux/wait.h>
long wait_event_timeout(wait_queue_head_t q, condition, long timeout);
long wait_event_interruptible_timeout(wait_queue_head_t q, condition, long timeout);
```

These functions sleep on the given wait queue, but return after the timeout (in units of jiffies) expires. As a result, they implement a bounded sleep that does not go on forever. 

The /proc/jitqueue file shows a delay based on wait_event_interruptible_timeout:

```c
wait_queue_head_t wait;
init_waitqueue_head (&wait);
wait_event_interruptible_timeout(wait, 0, delay);
```

If you want to delay execution waiting for no specific event (like the code above), the kernel offers the schedule_timeout function so you don't need to bother with declaring an unneccessary queue head. The prototype is:

```c
#include <linux/sched.h>
signed long schedule_timeout(signed long timeout);
```

- `timeout` is the number of jiffies to delay
- The return value is 0 unless the function returns before the given timeout has elapsed (in response to a signal)
- `schedule_timeout` requires that the caller first set the current process state, so a typical call looks like:

```c
set_current_state(TASK_INTERRUPTIBLE);
schedule_timeout (delay);
```

#### Short Delays

When dealing with latencies in hardware, delays are usually a few dozen microseconds at most. In this case, using the clock tick is not the way to go. 

The kernel functions ndelay, udelay, and mdelay serve well for short delays, delaying execution for the specified number of nanoseconds, microseconds, or milliseconds respectively. Their prototypes are:

```c
#include <linux/delay.h>
void ndelay(unsigned long nsecs); //nano
void udelay(unsigned long usecs); //micro
void mdelay(unsigned long msecs); //milli
```

These functions can be found in `<asm/delay.h>`.

To avoid overflows, udelay and ndelay impose an upper bound in the value passed to them. To avoid errors, you should delay with the coarsest-grained resolution possible (use 1 milli instead of 1000 micro or even more nano). 

Remember that all three of these functions are busy-waiting functions, meaning that they are hogging the CPU while executing nothing. Another way to do these delays is with the sleep functions in `<linux/delay.h>`:

```c
void msleep(unsigned int millisecs);
unsigned long msleep_interruptible(unsigned int millisecs);
void ssleep(unsigned int seconds);
```

The first two functions put the calling process to sleep for the input number of millisecondss. A call to ssleep puts the process into an uninterruptible sleep for the given number of seconds.

In general, if you can tolerate longer delays than requested, you should be using schedule_timeout, msleep, and/or ssleep in driver code.

### Kernel Timers

If you need to schedule an action to happen later without blocking the current process until that time arrives, kernel timers are the right tool to use. These timers schedule execution of a function at a particular time in the future. 

A kernel timer is a data structure that instructs the kernel to execute a user-defined function with a user-defined argument at a user-defined time. It is defined in `<linux/timer.h>` and `kernel/timer.c`.

The processes scheduled to run are run asynchronously from the process that registered them. When a timer runs, the process that scheduled it could be asleep, executing on a different processor, or exited. 

Kernel timers are run as the result of a software interrupt. You need to be really careful in implementing them! A number of actions require the context of a process in order to be executed. When you are in interrupt context, you need to observe the following rules:

-  No access to user space is allowed. Because there is no process context, there is no path to the user space associated with any particular process.
-  The `current` pointer is not meaningful in atomic mode and cannot be used since the relevant code has no connection with the process that has been interrupted.
-  No sleeping or scheduling may be performed. Atomic code may not call schedule or a form of wait_event, nor may it call any other function that could sleep. For example, calling kmalloc(..., GFP_KERNEL) is against the rules. Semaphores also must not be used since they can sleep.

Kernel code can tell if it is running in interrupt context by calling the function `in_interrupt()` which has no parameters and returns nonzero if the processor is currently running in interrupt context (could be hardware or software interrupt). It's kind of like asking the kernel to pinch you to check if you are dreaming or not. 

Another function related to `in_interrupt()` is `in_atomic()`.  Its return value is nonzero whenever scheduling is not allowed. This includes hardware and software interrupt contexts as well as any time when a spinlock is held. In the spinlock case current may be valid, but access to user space is forbidden since it can cause scheduling to happen. Whenever you are using `in_interrupt()`, you should really consider if `in_atomic()` is what you actually mean. Both functions are declared in `<asm/hardirq.h>`. 

# GO OVER THESE DIFFERENCES ^^^

One other cool feature of kernel timers is that a task can reregister itself to run again at a later time. This is due to the fact that each `timer_list` structure is unlinked from the list of active timers before being run and can then by immediately relinked elsewhere. Rescheduling the same task over and over can be useful for polling. On an SMP system, a timer that reregisters itself always runs on the same CPU. 

### The Timer API

The kernel provides drivers with a number of functions to declare, register, and remove kernel timers. The following code shows the basic overview:

```c
#include <linux/timer.h>
struct timer_list {
     /* ... */
     unsigned long expires;
     void (*function)(unsigned long);
     unsigned long data;
};

void init_timer(struct timer_list *timer);
struct timer_list TIMER_INITIALIZER(_function, _expires, _data);

void add_timer(struct timer_list * timer);
int del_timer(struct timer_list * timer);
```

- The data structure includes more fields than the ones shown, but those three are the ones that are meant to be accessed from outside the timer code iteslf
- The `expires field` represents the jiffies value when the timer is expected to run
- When the jiffies value is reached, , the function `function` is called with `data` as an argument
- The structure must be initialized before use to ensure all fields are properly setup
- Initialization can be performed by calling init_timer or assigning TIMER_INITIALIZER to a static structure
- After initialization, you can change the three public fields before calling add_timer
- To disable a registered timer before it expires, call del_timer

Here are the rest of the timer API functions:

```
int mod_timer(struct timer_list *timer, unsigned long expires);
    Updates the expiration time of a timer, a common task for which a timeout
    timer is used. mod_timer can be called on inactive timers as well, where you 
    normally use add_timer.
    
int del_timer_sync(struct timer_list *timer);
    Works like del_timer, but also guarantees that when it returns, the timer 
    function is not running on any CPU. del_timer_sync is used to avoid race 
    conditions on SMP systems and is the same as del_timer in UP kernels. This 
    function should be preferred over del_timer in most situations. This 
    function can sleep if it is called from a nonatomic context but busy waits 
    in other situations. If the timer function reregisters itself, the caller 
    must first ensure that this reregistration will not happen; this is usually 
    accomplished by setting a “shutting down” flag, which is checked by the 
    timer function.
    
int timer_pending(const struct timer_list * timer);
    Returns true or false to indicate if the timer is currently scheduled  
    to run by reading one of the opaque fields of the structure.
```

### The Implementation of Kernel Timers

The implementation of timers has been done to meet the following conditions:

- Timer management must be as lightweight as possible
- The design should scale well as the number of active timers increases
- Most timers expire within a few seconds or minutes at most, while timers with long delays are pretty rare
- A timer should run on the same CPU that registered it

Whenever kernel code registers a timer, the operation is eventually performed by internal_add_timer which then adds the new timer to a double-linked list of timers within a cascading table associated to the current CPU (whew that's a lot of steps for a simple timer!).

How the cascading table works:

- If the timer expires in the next 0 to 255 jiffies, it is added to one of the 256 lists devoted to short-range timers using the least significant bits of the `expires` field
- If it expires farther in the future, it is added to one of 64 lists based on bits 9–14 of the `expires` field
- For timers expiring even farther in time, the same trick is used for bits 15–20, 21–26, and 27–31
- Timers with an expire field pointing still farther in the future are hashed with a delay value of 0xffffffff
- timers with `expires` in the past are scheduled to run at the next timer tick
- When `__run_timers` is fired, it executes all pending timers for the current timer tick
- If jiffies is currently a multiple of 256, the function also rehashes one of the next-level lists of timers into the 256 short-term lists

# Go over this process, it sounds cool ^^^

Note: If you need really accurate timing for industrial environments, you will probably need a real-time kernel extension. 

### Tasklets

The tasklet mechanism is often used in interrupt management. They resemble kernel timers in some ways:

- They are always run at interrupt time
- They always run on the same CPU that schedules them
- They receive an unsigned long argument

However, they are different in these ways:

- You can’t ask to execute the function at a specific time
- You simply ask for it to be executed at a later time chosen by the kernel

A tasklet exists as a data structure and (just like with kernel timers) must be initialized before use. Initialization can be performed by calling a specific function or by declaring the structure using macros:

```c
#include <linux/interrupt.h>

struct tasklet_struct {
 /* ... */
    void (*func)(unsigned long);
    unsigned long data;
};

void tasklet_init(struct tasklet_struct *t,
    void (*func)(unsigned long), unsigned long data);
    
DECLARE_TASKLET(name, func, data);
DECLARE_TASKLET_DISABLED(name, func, data);
```

Cool features of tasklets:

- A tasklet can be disabled and re-enabled later. It won’t be executed until it is enabled as many times as it has been disabled. (???)
- Just like timers, a tasklet can reregister itself.
- A tasklet can be scheduled to execute at normal priority or high priority. The high priority group is always executed first.
- Tasklets may be run immediately if the system is not under heavy load but never later than the next timer tick.
- A tasklets can be concurrent with other tasklets but is strictly serialized with respect to itself. The same tasklet never runs simultaneously on more than one processor. A tasklet always runs on the same CPU that schedules it.

The following list describes the kernel interface to tasklets
after the tasklet structure has been initialized:

```
void tasklet_disable(struct tasklet_struct *t);
  This function disables the given tasklet. The tasklet may still be scheduled 
  with tasklet_schedule, but its execution is deferred until the tasklet has 
  been enabled again.

void tasklet_disable_nosync(struct tasklet_struct *t);
  Disable the tasklet, but without waiting for any currently-running function to
  exit. When it returns, the tasklet is disabled and won’t be scheduled in the 
  future until re-enabled, but it may be still running on another CPU when the 
  function returns.

void tasklet_enable(struct tasklet_struct *t);
  Enables a tasklet that had been previously disabled. If the tasklet has 
  already been scheduled, it will run soon.

void tasklet_schedule(struct tasklet_struct *t);
  Schedule the tasklet for execution. If a tasklet is scheduled again before it 
  has a chance to run, it runs only once. However, if it is scheduled while it 
  runs, it runs again after it completes. This ensures that events occurring 
  while other events are being processed receive due attention. It also allows a 
  tasklet to reschedule itself.

void tasklet_hi_schedule(struct tasklet_struct *t);
  Schedule the tasklet for execution with higher priority. Runs before the low 
  priority tasklets. 

void tasklet_kill(struct tasklet_struct *t);
  This function ensures that the tasklet is not scheduled to run again; it is 
  usually called when a device is being closed or the module removed. 
```

Tasklets are implemented in `kernel/softirq.c`. The data structure used in tasklet management is a simple linked list because tasklets have none of the sorting requirements that kernel timers have.

### Workqueues

Workqueues are similar to tasklets but differ in the following ways:

- Tasklets run in software interrupt context with the result that all tasklet code must be atomic. Instead, workqueue functions run in the context of a special kernel process; as a result, they have more flexibility. In particular, workqueue functions can sleep.
- Tasklets always run on the processor from which they were originally submitted. Workqueues work in the same way, by default.
- Kernel code can request that the execution of workqueue functions be delayed for an explicit interval.
- KEY DIFFERENCE: Tasklets execute quickly for a short period of time, and in atomic mode. Workqueue functions may have higher latency but need not be atomic. Each mechanism has appropriate use cases.

Workqueues have a type of `struct workqueue_struct` defined in `<linux/workqueue.h>`. They must be created before use with one of the following:

```c
struct workqueue_struct *create_workqueue(const char *name);
struct workqueue_struct *create_singlethread_workqueue(const char *name);
```

- `create_workqueue` will get a workqueue that has a dedicated thread for each processor on the system
- `create_singlethread_workqueue` is for when a single working will suffice

To submit a task to a workqueue, you need to fill in a `work_struct` structure with the following:

```c
DECLARE_WORK(name, void (*function)(void *), void *data);
```

- `name` is the name of the structure to be declared
- `function` is the function to be called from the workqueue
- `data` is a value to pass to that function

If you need to set up the work_struct structure at runtime, use the following two macros:

```c
INIT_WORK(struct work_struct *work, void (*function)(void *), void *data);
PREPARE_WORK(struct work_struct *work, void (*function)(void *), void *data);
```

- `INIT_WORK` should be used the first time the structure is set up
- `PREPARE_WORK` does not initialize the pointers used to link the work_struct structure into the workqueue. Use this one if the structure may currently be submitted to a workqueue and you need to change that structure.

There are two functions for submitting work to a workqueue:

```c
int queue_work(struct workqueue_struct *queue, struct work_struct *work);
int queue_delayed_work(struct workqueue_struct *queue,
                struct work_struct *work, unsigned long delay);
```

- Both add work to a given queue
- queue_delayed_work delays the work until at least delay jiffies have passed
- Both functions return 0 if successful

If you need to cancel a pending workqueue entry, you may call:

```c
int cancel_delayed_work(struct work_struct *work);
```

The return value is nonzero if the entry was canceled before it began execution. The kernel guarantees that execution of the given entry will not be initiated after a call to cancel_delayed_work. 

If cancel_delayed_work returns 0 the entry may have already been running on a different processor, and might still be running after a call to cancel_delayed_work. 

To be 100% sure that the work function is not running anywhere in the system after cancel_delayed_work returns 0, you must follow that call with a call to:

```c
void flush_workqueue(struct workqueue_struct *queue);
```

After flush_workqueue returns, no work function submitted prior to the call is running anywhere in the system. When done with the workqueue, get rid of it with:

```c
void destroy_workqueue(struct workqueue_struct *queue);
```

# Go over the basics of workqueues ^^^

### The Shared Queue

Most of the time a device driver does not need its own workqueue. If you only submit tasks to the queue occasionally, it may be more efficient to simply use the shared, default workspace that is provided by the kernel. Note that you should not be hogging this space and that other people will affect how long it takes your tasks to get their turn on the CPU. 

# GO over the actual code to implement this in the book and which parts are relevant






